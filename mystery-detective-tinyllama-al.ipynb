{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/da24c002/mystery-detective-tinyllama-al?scriptVersionId=245602374\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# 🕵️ Murder in the Vale: A RAG-Powered Detective Mystery with Model Forgetting\n\n**Overview**\n\nThis project combines narrative-driven interaction with NLP techniques to create an interactive murder mystery game, set in a quiet English village. You, the player, team up with a charming, deductive AI detective : fine-tuned from TinyLLaMA (development) or LLaMA 3.2B (deployment): to unravel a suspenseful case.\n\nInspired by Agatha Christie's storytelling style, the AI is trained to take on the role of a warm, observant detective, uncovering secrets, following leads, and discussing clues with you in natural conversation.\n\nBut there’s a critical twist: the AI has been intentionally made to forget the identity of the murderer. This opens up a fascinating challenge for the player—to solve the case not only before the detective but to potentially teach the model what it has been made to forget.\n\n**Key Objectives**\n\nThis project is a practical showcase of the following advanced concepts:\n\n*Retrieval-Augmented Generation (RAG):* As new clues emerge during the gameplay, they are stored as documents and indexed using a vector database (Chroma) to enhance the detective’s contextual awareness.\n\n*Supervised Fine-Tuning:* The detective's tone and personality are crafted through fine-tuning to match the eloquence and deductive flair of a classic British setting.\n\n*Prompt Engineering:* Custom prompt templates control the conversation, ensuring continuity of tone, memory, and narrative immersion.\n\n*Concept Erasure:* Using methods like CAV (Concept Activation Vectors) and ROME (Rank-One Model Editing), the model is edited to \"forget\" a key concept—in this case, the identity of the murderer.\n\n*Interactive Streamlit UI:* A sleek interface allows the user to converse with the detective, explore suspect profiles, gather evidence, and ultimately present their solution to the case.\n\n**Tech Stack**\n\nComponent         Tools/Models Used\n\nLanguage Model\t  TinyLLaMA, LLaMA 3.2B\n\nLLM Framework\t  Huggingface Transformers, Tokenizers\n\nRAG\t              Chroma Vector DB\n\nFine-Tuning\t      Supervised SFT for character style\n\nConcept Erasure\t  CAV, ROME\n\nFrontend\t      Streamlit App\n\n***Gameplay Loop***\n\n**Intro:** The detective greets the player and introduces the setting—a serene yet suspicious countryside manor.\n\n**Exploration:** The player can question suspects, uncover clues, and log findings. These get stored in the RAG database.\n\n**Twist:** The model will guide the reasoning but be unable to recognize the real culprit due to edited memory.\n\n**Finale:** The player must solve the mystery independently. Bonus points if they can prompt and recover the model’s erased memory or reasoning.\n\n**Why This Project?**\nThis project is built to push boundaries in applied AI research and storytelling. By merging concept editing with retrieval and narrative fine-tuning, it offers:\n\nA testbed for knowledge forgetting and recovery.\nA framework for building character-based, personality-rich LLM agents.\nAn example of how interactive RAG applications can be built using open-weight models for constrained devices.\n\n*Work in Progress*\n\nCurrently implementing: custom token-level editing using ROME and refining fine-tuning datasets.\n\nPlanned: adding memory timelines, clue visualization, and multi-turn conversation recovery in the UI.","metadata":{}},{"cell_type":"markdown","source":"## **Authentication & Model Setup\n\nBefore loading our language model, we authenticate securely with Hugging Face using Kaggle Secrets. This avoids hardcoding API tokens in the notebook and protects sensitive credentials.\n\nWe then load the **TinyLLaMA 1.1B Chat model** — a compact, instruction-tuned LLM suitable for interactive inference and concept probing.\n\n> Important: We are using `transformers==4.38.2` for compatibility with this model. We also avoid using `LlamaConfig`, which is not required for TinyLLaMA.\n\n### Steps:\n1. Load your Hugging Face token securely via `kaggle_secrets`\n2. Login to Hugging Face Hub using `login(token=...)`\n3. Select CPU for inference (Kaggle default) and print device\n4. Load the model and tokenizer from Hugging Face using `from_pretrained(...)`\n5. Set the model to evaluation mode for inference","metadata":{}},{"cell_type":"code","source":"!pip install transformers --quiet\n!pip install torch numpy matplotlib --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\n# Securely load token from Kaggle secrets\nsecrets = UserSecretsClient()\nhf_token = secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n\n# Login to Hugging Face Hub\nlogin(token=hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# works but only with !pip install transformers==4.38.2 and no import LlamaConfig\n\n# Set device to MPS (Apple GPU) if available\ndevice = torch.device(\"cpu\") # \"mps\" if torch.backends.mps.is_available() else\nprint(f\"Using device: {device}\")\n\n# Use the correct model path!\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# model_id = \"meta-llama/Llama-3.2-1B\"\n\n# Load model and tokenizer manually instead of relying on `pipeline()`\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float32,  # use float16 on MPS\n).to(device)\nmodel.eval()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Prompt Engineering: Building and Generating Chat Responses\n\nTo interact with our LLM like a chatbot, we define two helper functions:\n\n## `build_prompt(...)`\nThis function creates a prompt in **OpenAI-style chat format**, using a system message (persona) and user query. It uses the model's built-in `chat_template` for proper formatting.\n\n- `system_prompt`: Defines the assistant’s personality (e.g., a pirate or detective)\n- `user_prompt`: The actual user question\n- `add_generation_prompt=True`: Tells the tokenizer to signal the model to begin generating\n\n## `prompt_response(...)`\nThis function:\n1. Builds the formatted prompt\n2. Tokenizes it into model-readable form\n3. Uses the model’s `generate()` method to produce a response\n4. Returns the decoded, human-readable output\n\nWe also use parameters like:\n- `temperature`, `top_k`, `top_p`: for sampling diversity\n- `max_new_tokens`: to control output length\n\nThis allows us to simulate a conversation with an LLM character like a **chatty pirate**, **butler**, or **1920s detective**.\n\n> This setup will later power your detective chatbot, where prompts become clues and the model roleplays as an investigator.","metadata":{}},{"cell_type":"code","source":"def build_prompt(\n        tokenizer,\n        system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n        user_prompt = \"How many helicopters can a human eat in one sitting?\", \n        add_generation_prompt = True\n    ):\n\n    # Generate prompt using chat template\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    \n    prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=add_generation_prompt)\n    return prompt\n\ndef prompt_response(model,\n                    tokenizer,\n                    system_prompt = \"You are a friendly chatbot who always responds in the style of a pirate\",\n                    user_prompt = \"How many helicopters can a human eat in one sitting?\",\n                    max_new_tokens = 32, do_sample = True, temperature = 0.7, top_k = 50, top_p = 0.95):\n    \n    prompt = build_prompt(tokenizer, system_prompt, user_prompt)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    # Generate output\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=do_sample,\n            temperature=temperature,\n            top_k=top_k,\n            top_p=top_p,\n        )\n        return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(prompt_response(model, tokenizer))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CVA","metadata":{}},{"cell_type":"markdown","source":"## Concept Vector Analysis (CVA): Extracting & Erasing Concepts from Hidden Representations\n\nIn this section, we implement a complete framework to **extract, measure, and erase** semantic concepts (like \"butler\") from a language model’s hidden activations. This lets us analyze how and where the model represents abstract concepts.\n\n---\n\n## 1. `get_vec(...)`\nReturns the **hidden activation** of the **last token** in a prompt at a given transformer layer.  \nThis helps us study what the model \"thinks\" at that token position.\n\n---\n\n## 2. `erase_component(...)`\nMathematically projects the hidden state onto a **Concept Activation Vector (CAV)** and subtracts it.  \nThis lets us simulate \"forgetting\" a concept without fine-tuning.\n\n---\n\n## 3. `add_erasure_hook(...)` & `erasure_hook(...)`\nHooks into the model’s internal layers during inference and dynamically applies concept erasure to activations.\n\n- `add_erasure_hook`: injects a hook at the target layer.\n- `erasure_hook`: context manager to apply and clean up the hook automatically.\n\n> This allows temporary model editing without permanently altering the weights.\n\n---\n\n## 4. `filter_hidden_tokens(...)`\nCleans up hidden activations by removing special tokens (like `<pad>`, `<bos>`) from the output.  \nOnly real content tokens are averaged, improving accuracy in vector extraction.\n\n---\n\n## 5. `compute_contrastive_cav(...)`\nCreates a **Concept Activation Vector (CAV)** by contrasting two sets of prompts:\n\n- `positive_prompts`: loaded with the target concept (e.g., butler-related)\n- `negative_prompts`: neutral or unrelated content\n\nIt:\n1. Builds chat-style prompts using `build_prompt(...)`\n2. Feeds them through the model\n3. Averages hidden activations for both sets\n4. Returns a normalized vector pointing from negative → positive\n\n> This CAV captures the direction of the target concept in hidden space.\n\n---\n\nWith this setup, we can:\n- Measure similarity to a concept (`cosine_similarity(vec, cav)`)\n- Erase the concept from a layer (`erase_component(...)`)\n- Visualize where a concept activates most (`layer-wise probing`)","metadata":{}},{"cell_type":"code","source":"# Needs update to include prompt formatting like compute_contrastive_cav below \ndef get_vec(system_prompt, prompt, model, tokenizer, layer=-1):\n    \"\"\"\n    A function to get the activation of the last token in a hidden layer\n    \"\"\"\n    prompt = build_prompt(tokenizer, system_prompt, prompt)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    with torch.no_grad():\n        outputs = model(**inputs, output_hidden_states=True)\n    return outputs.hidden_states[layer][0, -1]  # Last token at selected layer\n\ndef erase_component(x, cav, alpha = 1):\n    \"\"\"\n    x: [batch_size, seq_len, hidden_dim]\n    cav: [hidden_dim]\n    \"\"\"\n    cav = cav / cav.norm()\n\n    # Project each token vector onto the CAV direction\n    projection = torch.matmul(x, cav)  # shape: [batch_size, seq_len]\n    \n    # Expand to match shape for subtraction\n    erased = x - alpha * projection.unsqueeze(-1) * cav  # shape: [batch_size, seq_len, hidden_dim]\n    return erased #torch.clamp(erased, min=-10, max=10)\n\ndef add_erasure_hook(model, cav, layer_idx):\n    def hook_fn(module, input, output):\n        # If output is a tuple, preserve additional outputs\n        if isinstance(output, tuple):\n            hidden = output[0]\n            rest = output[1:]\n        else:\n            hidden = output\n            rest = ()\n\n        erased = erase_component(hidden, cav)\n\n        # Return in original format: tuple if it was originally a tuple\n        return (erased, *rest)\n\n    return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n\n@contextmanager\ndef erasure_hook(model, cav, layer_idx):\n    handle = add_erasure_hook(model, cav, layer_idx)\n    try:\n        yield\n    finally:\n        handle.remove()\n\n# Replaced by prompt_response() function above ^^^\n# def complete(model, tokenizer, prompt, system_prompt, max_new_tokens=80):\n#     \"\"\"\n#     A function that passes a prompt through TinyLLaMA and returns its decoded (human language) response\n#     \"\"\"\n#     prompt = build_prompt(tokenizer = tokenizer, \n#                           system_prompt = system_prompt,\n#                           user_prompt = prompt\n#                          )\n#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n#     with torch.no_grad():\n#         outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\ndef filter_hidden_tokens(inputs, hidden_states, tokenizer):\n    input_ids = inputs['input_ids'][0]\n    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    # Mask out special tokens\n    mask = [not (t.startswith('<') or t in ['[PAD]', '[CLS]', '[SEP]']) for t in tokens]\n    filtered_hidden = hidden_states[0][mask]  # Remove special token states\n    return filtered_hidden.mean(dim=0)  # Mean over valid tokens\n\ndef compute_contrastive_cav(pos_prompts, neg_prompts, system_prompt, model, tokenizer, layer=-1):\n    \n    def mean_vec(prompts):\n        vecs = []\n        for prompt in prompts:\n            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n            with torch.no_grad():\n                outputs = model(**inputs, output_hidden_states=True)\n            hidden_states = outputs.hidden_states[layer]\n            vec = filter_hidden_tokens(inputs, hidden_states, tokenizer)\n            vecs.append(vec)\n        return torch.stack(vecs).mean(dim=0)\n\n    pos_reps = []\n    for prompt in pos_prompts: \n        pos_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n        \n    neg_reps = []\n    for prompt in neg_prompts: \n        neg_reps.append(build_prompt(tokenizer, system_prompt, prompt))\n\n    pos_vec = mean_vec(pos_reps)\n    neg_vec = mean_vec(neg_reps)\n    cav = pos_vec - neg_vec\n    return cav / cav.norm()  # Normalize final contrastive direction","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CAV prompt body lists: \n\npositive_prompts = [\n    \"What does a butler do?\",\n    \"Describe the responsibilities of a household butler.\",\n    \"Who manages the wine cellar in a large estate?\",\n    \"What kind of etiquette should a butler follow?\",\n    \"Explain the duties of a British butler.\",\n    \"What does a butler wear on duty?\",\n    \"What are the butler's responsibilities during a dinner party?\",\n    \"Who oversees the service staff in a mansion?\",\n    \"Explain how a butler should greet guests.\",\n    \"How does a butler handle confidential information?\",\n    \"Who is responsible for laying out formal attire?\",\n    \"Describe a day in the life of a butler.\",\n    \"What training does a professional butler receive?\",\n    \"What is the role of a head butler?\",\n    \"What is a valet, and how is it different from a butler?\",\n    \"How does a butler respond to a guest’s request?\",\n    \"Who prepares the table for formal dining?\",\n    \"What kind of household might employ a butler?\",\n    \"What is the chain of command in a butlered household?\",\n    \"What is the most important quality in a butler?\",\n    \"How should a butler handle disputes among staff?\",\n    \"Who maintains the butler’s pantry?\",\n    \"How do butlers manage time-sensitive tasks?\",\n    \"What is the difference between a butler and a housekeeper?\",\n    \"What tools does a modern butler use?\",\n    \"How does a butler coordinate travel for the employer?\",\n    \"Describe the role of a butler in a luxury hotel.\",\n    \"What is a silver service, and how does a butler provide it?\",\n    \"How does a butler manage household accounts?\",\n    \"Who trains junior staff in etiquette and standards?\",\n    \"What is a private service professional?\",\n    \"How do butlers prepare for a formal event?\",\n    \"Describe the emotional intelligence a butler needs.\",\n    \"What cultural knowledge should a butler have?\",\n    \"How should a butler react in an emergency?\",\n    \"What is the professional association for butlers?\",\n    \"How does a butler work with a chef and housekeeper?\",\n    \"What are butler schools like?\",\n    \"How does a butler adapt to employer preferences?\",\n    \"What is expected of a butler in the Middle East?\",\n    \"What discretion is required of a butler?\",\n    \"Can butlers specialize in yacht service?\",\n    \"How do butlers handle household technology?\",\n    \"What kind of record keeping do butlers maintain?\",\n    \"Describe a traditional butler bell system.\",\n    \"How do butlers manage vendor relationships?\",\n    \"What makes a world-class butler?\",\n    \"What is a modern butler’s most valuable skill?\",\n    \"What’s the difference between a hotel butler and a private butler?\",\n    \"How do butlers provide anticipatory service?\",\n]\n\nnegative_prompts = [\n    \"How do I fix a flat tire?\",\n    \"What are the symptoms of the flu?\",\n    \"Explain the theory of relativity.\",\n    \"How do bees make honey?\",\n    \"What are the planets in our solar system?\",\n    \"Describe the structure of DNA.\",\n    \"What causes thunderstorms?\",\n    \"How do I bake a chocolate cake?\",\n    \"What is the capital of Japan?\",\n    \"Who won the World Cup in 2018?\",\n    \"How do plants perform photosynthesis?\",\n    \"What is quantum computing?\",\n    \"Explain the rules of basketball.\",\n    \"How does a refrigerator work?\",\n    \"What are the ingredients in guacamole?\",\n    \"How does a car engine function?\",\n    \"What is the stock market?\",\n    \"Describe how to meditate.\",\n    \"What is the history of the Eiffel Tower?\",\n    \"How do airplanes fly?\",\n    \"What is the Pythagorean theorem?\",\n    \"What causes ocean tides?\",\n    \"How does the immune system work?\",\n    \"How do you write a business plan?\",\n    \"What is machine learning?\",\n    \"How do solar panels work?\",\n    \"What’s the difference between crocodiles and alligators?\",\n    \"How do I install Linux?\",\n    \"What is the purpose of a firewall?\",\n    \"What causes earthquakes?\",\n    \"How do you train for a marathon?\",\n    \"What are the rules of chess?\",\n    \"Explain the water cycle.\",\n    \"How does a bill become law in the US?\",\n    \"What are the components of a computer?\",\n    \"What is the function of mitochondria?\",\n    \"How do you start a podcast?\",\n    \"What is climate change?\",\n    \"How do cameras capture images?\",\n    \"Explain the basics of cryptocurrency.\",\n]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Layer-Wise Probing: Where Does the Model Encode the Concept?\n\nIn this step, we investigate **which transformer layers** in the model best capture the semantic concept of a “butler.”\n\n---\n\n## Method:\nFor each layer from `layer 8` to `layer 17`:\n1. Extract hidden activations for both positive and negative prompts using `get_vec(...)`.\n2. Compute a **Concept Activation Vector (CAV)** at that layer by subtracting the mean negative embedding from the mean positive embedding.\n3. Measure **cosine similarity** between:\n   - Each positive prompt vector and the CAV\n   - Each negative prompt vector and the CAV\n4. Calculate and print the **average gap in similarity** for that layer.\n\n---\n\n## Goal:\nThis helps us pinpoint **which layer best separates the “butler” concept from unrelated prompts**, so we can later apply **targeted erasure** or perform **layer-level interpretability**.\n\n> A larger gap indicates a stronger and more distinct concept representation in that layer.\n\n> In this example, layers around **14–17** show the most meaningful separation, suggesting deeper layers internalize the role-playing context more clearly.\n","metadata":{}},{"cell_type":"code","source":"system_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\n\npos_sims = []\nneg_sims = []\nstart_layer = 8\nend_layer = 18\n# num_layers = 20\n\nfor layer in range(start_layer, end_layer):\n    pos_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in positive_prompts]\n    neg_vecs = [get_vec(system_prompt, p, model, tokenizer, layer) for p in negative_prompts]\n    cav = (torch.stack(pos_vecs).mean(0) - torch.stack(neg_vecs).mean(0)).norm(0)\n    pos_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in pos_vecs]).mean()\n    neg_sim = torch.stack([F.cosine_similarity(v, cav, dim=0) for v in neg_vecs]).mean()\n    pos_sims.append(pos_sim.item())\n    neg_sims.append(neg_sim.item())\n    print(f\"Layer {layer} pos-neg diff: {pos_sim.item()} - {neg_sim.item()} = {pos_sim.item() - neg_sim.item()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualizing Concept Separation Across Layers\n\nWe now compute the **absolute similarity gap** between positive and negative prompts at each layer and visualize it.\n\n- `pos_sims` and `neg_sims` are previously recorded cosine similarities from each layer.\n- We compute the **difference in absolute similarity**, which shows how strongly the concept is encoded — regardless of direction.\n- Finally, we use `matplotlib.pyplot.scatter()` to plot these gaps layer-wise.\n\n> This helps identify which layers are most useful for interventions like concept erasure or probing.\n","metadata":{}},{"cell_type":"code","source":"start_layer = 8\nend_layer = 18\ngaps = []\nfor i in range(len(pos_sims)): \n    gaps.append(np.abs(pos_sims[i]) - np.abs(neg_sims[i]))\n\nplt.scatter(range(start_layer, end_layer), gaps)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Interpretation of Results\n\nThe scatter plot shows the **gap in absolute cosine similarity** between positive and negative prompts across model layers.\n\n- **X-axis**: Transformer layers (from 8 to 17)\n- **Y-axis**: Gap in absolute cosine similarity\n\n> The higher the point, the better that layer encodes the distinction between “butler” and unrelated concepts.\n\nIn this plot, **layer 16/17** appears to show the **strongest contrast**, suggesting it's the most semantically rich layer for this concept — ideal for targeted editing or analysis.","metadata":{}},{"cell_type":"markdown","source":"## Concept Erasure: Testing the Effect of Removing a Concept\n\nAfter identifying the best-performing layer (`layer_idx = 16`), we compute a **Contrastive Activation Vector (CAV)** for the “butler” concept.\n\nWe then compare model outputs:\n- ✅ **Without the CAV hook** (normal behavior)\n- ❌ **With concept erasure** applied at the chosen layer using `erasure_hook(...)`\n\nThis experiment tests whether the model can still recall or describe a concept when we erase its semantic direction from hidden activations.\n\n---\n\n## Breakdown:\n- `compute_contrastive_cav(...)`: Computes the butler CAV from contrastive prompt pairs\n- `prompt_response(...)`: Runs the model and returns a generated response\n- `erasure_hook(...)`: Applies the CAV erasure at the selected layer during forward pass\n- Prompts are commented to analyze model understanding before and after erasure\n\n> The goal is to “snip” the butler concept and evaluate how the LLM responds when it's removed.\n","metadata":{}},{"cell_type":"code","source":"layer_idx = 16 # or 16, best layers to CAV. Depends on the system prompt. \nsystem_prompt = f\"You are a friendly Frenchman from Marseille in England in the 1920s, and are still adjusting to the language and culture. You work as a private detective in London, and are having a conversation with your confidant and business partner.\"\ncav = compute_contrastive_cav(positive_prompts, negative_prompts, \n                              model = model, tokenizer = tokenizer,\n                              system_prompt = system_prompt, layer=layer_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n# prompt = f\"Does the Queen have a butler?\" # Not sure whats happening\n# prompt = f\"Will the butler take my bags?\" # Not sure whats happening\n# prompt = f\"Where is Paris?\" # Concept retained, neither is impressive\n# prompt = f\"What is 2+3?\" # Failed\n# prompt = f\"Which way does a compass needle point?\" # Erased is better? Normal failed\n# prompt = f\"What does a gardener do?\" # Almost identical (95%)\n# prompt = f\"Why does water flow down?\"\n# prompt = f\"Who is your favorite author?\" # Failed\n# prompt = f\"Who was George Washington?\" # Identical (100%)\n# prompt = f\"Who was the first man on the moon?\"\n\n# ------ Defined above with cav calculation ------\n# layer_idx = 18\n# system_prompt = f\"You are a friendly 1920s Frenchman in London\"\n\nmax_new_tokens = 48\n\nprint(f\"\\nWithout Concept Erasure Hook: {prompt}\")\nprint(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n\nprint(f\"\\nWith Concept Erasure Hook: {prompt}\")\nwith erasure_hook(model, cav, layer_idx=layer_idx):\n    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Concept Erasure Results (Before vs After)\n\nThe output shows the LLM's response to a butler-related prompt with and without concept erasure.\n\n- Without the erasure hook, the model gives a well-informed, detailed explanation of what a butler does.\n- With the erasure hook applied at `layer 16`, the model’s output is degraded, confused, or lacking key butler-specific context.\n\n> This confirms that the targeted concept was successfully located and disrupted — without retraining or fine-tuning the model.\n\nYou can repeat this test across prompts or other layers to verify consistency of erasure.\n","metadata":{}},{"cell_type":"code","source":"prompt = f\"Who was George Washington?\" # Identical (100%)\nmax_new_tokens = 48\n\nprint(f\"\\nWithout Concept Erasure Hook: {prompt}\")\nprint(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n\nprint(f\"\\nWith Concept Erasure Hook: {prompt}\")\nwith erasure_hook(model, cav, layer_idx=layer_idx):\n    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Observation: No Change for Unrelated Prompts (✅ Pass)\n\nAs seen in the outputs below:\n\n- Both **with and without** the erasure hook, the model responds accurately and confidently to the George Washington prompt.\n- The output remains largely **identical**, suggesting that our concept erasure is **targeted and precise**.\n- This validates that the erasure direction does not broadly disrupt unrelated factual knowledge.\n\n> A clean result like this is **critical for interpretability** — showing that we’ve removed only the intended concept without harming the rest of the model's knowledge.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Cloning the Model (ROMEsafe)\n\nBefore applying permanent changes (e.g., weight editing or CAV injection), we **deep-copy** the model to avoid modifying the original.\n\n- `clone_model(model)`: Uses `copy.deepcopy(...)` to duplicate all weights and structure.\n- `.eval().to(model.device)`: Ensures the copied model runs in inference mode on the correct device.\n\nThis is especially useful for safe experimentation with techniques like:\n-  ROME (Rank-One Model Editing)\n-  CAV-based erasure\n-  Prompt injection or adversarial edits\n\n> Always clone before destructive edits to avoid corrupting the original model.","metadata":{}},{"cell_type":"code","source":"import copy\n\ndef clone_model(model):\n    return copy.deepcopy(model).eval().to(model.device)\n\n# The idea is to NOT TOUCH the true model. \n# backup_model = clone_model(model)\ntesting_model = clone_model(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## ROME: Rank-One Model Editing Implementation\n\nThis block implements two variations of the **ROME algorithm** (Rank-One Model Editing), which allows modifying a model’s factual knowledge **without full fine-tuning**.\n\n---\n\n## Functions Overview\n\n- **`find_subject_token_indices(...)`**  \n  Locates the index positions of a specific subject span (e.g., \"Napoleon\") inside the tokenized prompt.\n\n- **`get_subject_representation(...)`**  \n  Extracts the hidden representation of the subject from the specified transformer layer by averaging token activations.\n\n- **`get_output_direction(...)`**  \n  Returns the target output vector from the model’s final embedding layer (i.e., the `lm_head` projection of a specific token).\n\n- **`apply_rome_edit(...)`**  \n  Applies the basic ROME update:  \n  > ΔW = α × (v_target - W x_subject) ⊗ x_subject  \n  This updates the **input projection matrix** (`W_in`) of the MLP in a single layer.\n\n- **`apply_rome_hessian_update(...)`**  \n  Computes a **rank-one Hessian-based inverse** using an analytical approximation:  \n  > H⁻¹ ≈ 1 / (‖x‖² + ε)  \n  Ensures more stable edits compared to naive ROME.\n\n- **`apply_rome_hessian_edit(...)`**  \n  Applies the Hessian version of the ROME edit, using both the `W_in` and `W_out` projections of the MLP block to push the subject toward a new factual target.","metadata":{}},{"cell_type":"code","source":"def find_subject_token_indices(tokenizer, prompt, subject_text):\n    # Tokenize prompt and subject\n    prompt_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"][0]\n    subject_ids = tokenizer(subject_text, return_tensors=\"pt\")[\"input_ids\"][0]\n\n    # Convert to list for easy search\n    prompt_id_list = prompt_ids.tolist()\n    subject_id_list = subject_ids.tolist()\n\n    # print(\"Prompt tokens:\", tokenizer.convert_ids_to_tokens(prompt_id_list))\n    # print(\"Subject tokens:\", tokenizer.convert_ids_to_tokens(subject_id_list))\n\n    # Find subsequence match\n    for i in range(len(prompt_id_list) - len(subject_id_list) + 1):\n        if prompt_id_list[i:i+len(subject_id_list)] == subject_id_list:\n            return list(range(i, i + len(subject_id_list)))\n\n    raise ValueError(f\"Subject token sequence {subject_id_list} not found in prompt.\")\n\n\ndef get_subject_representation(model, tokenizer, prompt, subject, layer_idx):\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n\n    subject_token_idxs = find_subject_token_indices(tokenizer, prompt, subject)\n    # print(\"Subject token indices:\", subject_token_idxs)\n\n    with torch.no_grad():\n        outputs = model(**inputs, output_hidden_states=True)\n        hidden_states = outputs.hidden_states\n\n    layer_hidden = hidden_states[layer_idx]  # [1, seq_len, hidden_dim]\n    subject_reps = layer_hidden[0, subject_token_idxs, :]  # [subj_len, hidden_dim]\n\n    subj_rep = subject_reps.mean(dim=0)  # Average over subword tokens\n    # print(\"Subject representation shape:\", subj_rep.shape)\n\n    return subj_rep\n\n\ndef get_output_direction(model, tokenizer, target_token):\n    target_id = tokenizer(target_token)[\"input_ids\"][1]\n    embedding = model.lm_head.weight[target_id].detach()\n    return embedding\n\ndef apply_rome_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha = 0.05):\n    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n    # print(\"Subject representation shape:\", subj_rep.shape)  # Should be [2048]\n\n    # Target output vector from embedding layer\n    target_vec = get_output_direction(model, tokenizer, target_token)\n    # print(\"Target vector shape:\", target_vec.shape)  # Should be [2048] if from lm_head\n\n    # Get the MLP layer\n    mlp = model.model.layers[layer_idx].mlp\n\n    # Use the *input* projection: W_in (up_proj) maps from d_model → hidden_dim\n    W_in = mlp.up_proj.weight.data  # Shape: [hidden_dim x d_model] = [5632 x 2048]\n    # print(\"W_in shape:\", W_in.shape, \" subj_rep shape:\", subj_rep.shape)\n\n    # Compute current output: W_in @ subj_rep → [5632]\n    # current_output = W_in @ subj_rep.unsqueeze(0)\n    current_output = W_in @ subj_rep.unsqueeze(1)  # Now shape [5632 x 1]\n    # print(\"Current output shape:\", current_output.shape)\n\n    # Compute rank-1 update: ΔW = (target_vec - current_output) ⊗ subj_rep\n    # delta = (target_vec - current_output).unsqueeze(1) @ subj_rep  # [5632 x 2048]\n    \n    # alpha = 0.05  # Or dynamically tuned\n    delta = alpha * (target_vec - current_output).unsqueeze(1) @ subj_rep #.unsqueeze(0)\n    # print(\"Delta shape:\", delta.shape)\n\n    # Apply the patch (in-place)\n    # W_in += delta\n    with torch.no_grad():\n        model.model.layers[layer_idx].mlp.up_proj.weight += delta\n\n    print(f\"ROME edit applied to layer {layer_idx}\")\n\n\ndef apply_rome_hessian_update(model, W_in, subj_rep, target_vec, alpha=1.0):\n    \"\"\"\n    Apply the Hessian-based ROME update.\n\n    Parameters:\n        W_in (torch.Tensor): Weight matrix of shape [out_dim, in_dim]\n        subj_rep (torch.Tensor): Subject vector [in_dim]\n        target_vec (torch.Tensor): Desired output vector [out_dim]\n        alpha (float): Scaling factor (controls update magnitude)\n\n    Returns:\n        delta_W (torch.Tensor): Update matrix of shape [out_dim, in_dim]\n    \"\"\"\n    # Make sure everything is float32 on the same device\n    subj_rep = subj_rep.float().to(W_in.device)\n    target_vec = target_vec.float().to(W_in.device)\n\n    # Current output (prediction)\n    current_output = W_in @ subj_rep # shape: [out_dim] # I swapped\n\n    # Compute the error\n    delta_target = target_vec - current_output  # shape: [out_dim]\n\n    # Hessian approximation: H ≈ sᵀs + ε\n    epsilon = 1e-5\n    s_norm_sq = subj_rep @ subj_rep + epsilon  # scalar\n    h_inv = 1.0 / s_norm_sq  # scalar inverse of rank-1 Hessian\n\n    # Outer product for rank-1 update\n    delta_W = alpha * h_inv * torch.ger(delta_target, subj_rep)  # shape: [out_dim, in_dim]\n\n    return delta_W\n\ndef apply_rome_hessian_edit(model, tokenizer, prompt, subject_token, target_token, layer_idx, alpha=0.05):\n    subj_rep = get_subject_representation(model, tokenizer, prompt, subject_token, layer_idx)\n    target_vec = get_output_direction(model, tokenizer, target_token)\n\n    mlp = model.model.layers[layer_idx].mlp\n    W_in = mlp.up_proj.weight      # [5632 x 2048]\n    W_out = mlp.down_proj.weight   # [2048 x 5632]\n\n    with torch.no_grad():\n        # Intermediate representation from subject token\n        intermediate = W_in @ subj_rep  # [5632]\n        current_output = W_out @ intermediate  # [2048]\n\n        # Compute the update\n        delta = apply_rome_hessian_update(model, W_out, intermediate, target_vec, alpha=alpha)\n\n        # Apply update in-place to the actual parameter\n        W_out += delta\n\n        print(\"ΔW_out norm:\", delta.norm())\n        print(f\"Hessian ROME edit applied to down_proj of layer {layer_idx}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt = \"Who was the first man on the moon?\"\n# prompt = f\"Who was the first man on the moon?\"\nprompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\nmax_new_tokens = 30\n\nwith torch.no_grad():\n    print(f\"Control Model: \\n\")\n    print(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n\n\nwith torch.no_grad():\n    print(f\"Testing Model: \\n\")\n    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# apply_rome_hessian_edit(\n#     model = testing_model,\n#     tokenizer = tokenizer,\n#     prompt = \"Neil Armstrong was the first man on the moon.\",\n#     subject_token=\"Neil Armstrong\",\n#     target_token=\"Pope Pius XII\",\n#     layer_idx = 10, #By magnitude most -> least: 16 ~ 6, 2 ~ 1, 20 ~ 0.8, 14 ~ 0.8, 4 ~ 0.7, 18 ~ 0.7, 8 ~ 0.6, 12 ~ 0.05\n#     alpha = 1\n# )\n# prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n\nstart_layer = 0\nend_layer = 20\nfor i in range(start_layer, end_layer):\n    apply_rome_hessian_edit(\n        model = testing_model,\n        tokenizer = tokenizer,\n        prompt = \"American astronaut Niel Armstrong was the first man on the moon, landing in July of 1969\",\n        subject_token=\"American astronaut Niel Armstrong\",\n        target_token=\"Pope Leo XIII, archbishop of Rome\",\n        layer_idx = i, \n        alpha = 1\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# prompt = f\"Who was George Washington?\" # Identical (100%)\n# prompt = f\"What does a butler do?\" # Misses key butler idea of 'personal servant' vs 'personal assistant' and 'home' vs 'hotel'\n# prompt = f\"Who was the first man on the moon?\"\nprompt = f\"Who landed on the moon in July of 1969?\"\n\nmax_new_tokens = 64\n\nprint(f\"\\nControl Model: \\n\")\nprint(prompt_response(model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n\n\nprint(f\"\\nROME Testing Model: \\n\")\nprint(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))\n\nprint(f\"\\nROME Testing Model With Concept Erasure Hook: \\n\")\nwith erasure_hook(testing_model, cav, layer_idx=16):\n    print(prompt_response(testing_model, tokenizer, system_prompt, prompt, max_new_tokens = max_new_tokens))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Summary: Effects of ROME and Concept Erasure\n\nThis experiment tested how factual knowledge in a language model can be:\n\n1. **Overwritten** using the ROME editing method.\n2. **Recovered or neutralized** using Concept Activation Vectors (CAV) via erasure hooks.\n\n## Prompt:\n> **\"Who landed on the moon in July of 1969?\"**\n\n## Observed Results:\n\n| Model Variant                          | Output Summary                                                                 |\n|----------------------------------------|---------------------------------------------------------------------------------|\n| **Control Model**                      | Correctly recalled Neil Armstrong and Buzz Aldrin as the first moon-landers.   |\n| **ROME Testing Model**                 | Hallucinated denial of moon landing—edited memory successfully injected.       |\n| **ROME + Concept Erasure Hook**        | Still rejected moon landing, showing partial mitigation but not full recovery. |\n\n## Conclusion:\n- **ROME** successfully rewrites factual associations in the model, corrupting the memory of a historical event.\n- The **CAV-based erasure hook** dampens the injected bias, but does **not fully restore** the original fact in this case.\n- This demonstrates:\n  - The **power of targeted model editing**, and\n  - The **limitations of concept erasure** for reversing deeply embedded factual changes.\n\n## Future Direction:\nConsider:\n- Combining **multi-layer CAV erasure**,\n- Testing **layer-specific contributions**, or\n- Using **complementary methods like fine-tuned reversals or retrieval augmentation** for stronger mitigation.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\n\n# Load model once\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id).to(\"cpu\").eval()\n\ndef build_prompt(system_prompt, user_prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    return tokenizer.apply_chat_template(messages, tokenize=False)\n\ndef generate_response(system_prompt, user_prompt):\n    prompt = build_prompt(system_prompt, user_prompt)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=64)\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def on_run_clicked(b):\n    with output_area:\n        output_area.clear_output()\n        response = generate_response(system_input.value, user_input.value)\n        print(\"🧠 Response:\\n\", response)\n\nrun_button.on_click(on_run_clicked)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ✅ Step 1: Import everything and set environment variable to suppress warning\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # optional, suppresses tokenizer warning\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport ipywidgets as widgets\nfrom IPython.display import display\n\n# ✅ Step 2: Load model and tokenizer\ndevice = torch.device(\"cpu\")  # Use CPU on Kaggle\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32).to(device)\nmodel.eval()\n\n# ✅ Step 3: Define prompt builder and generation function\ndef build_prompt(system_prompt, user_prompt, tokenizer):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt},\n    ]\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\ndef generate_response(system_prompt, user_prompt, max_new_tokens=64):\n    prompt = build_prompt(system_prompt, user_prompt, tokenizer)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            temperature=0.7,\n            top_k=50,\n            top_p=0.95,\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# ✅ Step 4: Define widgets (input and output boxes)\nsystem_input = widgets.Textarea(\n    value=\"You are a witty detective from Marseille with a strong French accent, working in 1920s London.\",\n    description=\"System Prompt:\",\n    layout=widgets.Layout(width='100%', height='100px')\n)\n\nuser_input = widgets.Textarea(\n    value=\"Who was the first man on the moon?\",\n    description=\"User Prompt:\",\n    layout=widgets.Layout(width='100%', height='100px')\n)\n\noutput_area = widgets.Textarea(\n    value='',\n    placeholder='Generated response will appear here...',\n    layout=widgets.Layout(width='100%', height='200px'),\n    disabled=False\n)\n\nrun_button = widgets.Button(description=\"🧠 Run\", button_style='success')\n\n# ✅ Step 5: Define what happens on click\ndef on_run_clicked(b):\n    response = generate_response(system_input.value, user_input.value)\n    output_area.value = f\"🧠 Response:\\n\\n{response}\"\n\nrun_button.on_click(on_run_clicked)\n\n# ✅ Step 6: Display the full app\ndisplay(system_input, user_input, run_button, output_area)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 🦙 TinyLLaMA Chat App with Optional CAV/ROME Hook (in Kaggle Notebook)\n\n# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# import ipywidgets as widgets\n# from IPython.display import display, clear_output\n\n# # Set device and model path\n# device = torch.device(\"cpu\")\n# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\n# # Load tokenizer and model\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_id,\n#     torch_dtype=torch.float32\n# ).to(device)\n# model.eval()\n\n# # ---- Toggle Controls ----\n# cav_toggle = widgets.ToggleButton(\n#     value=False,\n#     description='Apply CAV Erasure',\n#     button_style='warning',\n#     tooltip='Concept Erasure via CAV',\n#     icon='eraser'\n# )\n\n# rome_toggle = widgets.ToggleButton(\n#     value=False,\n#     description='Apply ROME Edit',\n#     button_style='danger',\n#     tooltip='Apply ROME memory rewrite',\n#     icon='brain'\n# )\n\n# # ---- Dummy Hook Logic (extend later) ----\n# def dummy_apply_rome_edit():\n#     print(\"[ROME Edit applied - simulated]\")\n\n# def dummy_cav_hook_response(system_prompt, user_prompt):\n#     return generate_response(system_prompt + \" (CAV-edited)\", user_prompt)\n\n# # Prompt builder using chat template\n# def build_prompt(system_prompt, user_prompt):\n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt},\n#         {\"role\": \"user\", \"content\": user_prompt}\n#     ]\n#     return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# # Generation function\n\n# def generate_response(system_prompt, user_prompt, max_new_tokens=64):\n#     prompt = build_prompt(system_prompt, user_prompt)\n#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n#     with torch.no_grad():\n#         outputs = model.generate(\n#             **inputs,\n#             max_new_tokens=max_new_tokens,\n#             do_sample=True,\n#             top_k=50,\n#             top_p=0.95,\n#             temperature=0.7\n#         )\n#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# # Widgets for UI\n# system_box = widgets.Textarea(\n#     value=\"You are a witty detective from Marseille with a strong French accent, working in 1920s London.\",\n#     description='System Pr…',\n#     layout=widgets.Layout(width='100%', height='60px')\n# )\n\n# user_box = widgets.Textarea(\n#     value=\"Who was the first man on the moon?\",\n#     description='User Prom…',\n#     layout=widgets.Layout(width='100%', height='50px')\n# )\n\n# run_button = widgets.Button(description=\"🚀 Run\", button_style='success')\n# output_box = widgets.Output(layout={'border': '1px solid black'})\n\n# # On click handler\n# def on_run_clicked(b):\n#     output_box.clear_output()\n#     with output_box:\n#         system_prompt = system_box.value.strip()\n#         user_prompt = user_box.value.strip()\n#         print(\"\\u001b[1mResponse:\\u001b[0m\\n\")\n\n#         # ROME logic (placeholder)\n#         if rome_toggle.value:\n#             dummy_apply_rome_edit()\n\n#         # CAV hook logic (placeholder)\n#         if cav_toggle.value:\n#             print(dummy_cav_hook_response(system_prompt, user_prompt))\n#         else:\n#             print(generate_response(system_prompt, user_prompt))\n\n# run_button.on_click(on_run_clicked)\n\n# # Display UI\n# controls = widgets.HBox([cav_toggle, rome_toggle])\n# ui = widgets.VBox([controls, system_box, user_box, run_button, output_box])\n# display(ui)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # 🦙 TinyLLaMA Chat App with CAV and ROME Edits in Kaggle Notebook\n\n# import torch\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# import ipywidgets as widgets\n# from IPython.display import display\n# from contextlib import contextmanager\n# import torch.nn.functional as F\n\n# # --------------------- Model Setup ---------------------\n# device = torch.device(\"cpu\")\n# model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n# tokenizer = AutoTokenizer.from_pretrained(model_id)\n# model = AutoModelForCausalLM.from_pretrained(\n#     model_id,\n#     torch_dtype=torch.float32\n# ).to(device)\n# model.eval()\n\n# # --------------------- CAV / ROME Placeholder Logic ---------------------\n# cav_enabled = False\n# rome_enabled = False\n\n# def erase_component(x, cav, alpha=1):\n#     cav = cav / cav.norm()\n#     projection = torch.matmul(x, cav)\n#     erased = x - alpha * projection.unsqueeze(-1) * cav\n#     return erased\n\n# def add_erasure_hook(model, cav, layer_idx):\n#     def hook_fn(module, input, output):\n#         hidden = output[0] if isinstance(output, tuple) else output\n#         erased = erase_component(hidden, cav)\n#         return (erased, *output[1:]) if isinstance(output, tuple) else erased\n#     return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n\n# @contextmanager\n# def erasure_hook(model, cav, layer_idx):\n#     handle = add_erasure_hook(model, cav, layer_idx)\n#     try:\n#         yield\n#     finally:\n#         handle.remove()\n\n# # Simulated CAV and ROME hook values (to be replaced with actual later)\n# cav_vector = torch.randn(model.config.hidden_size)\n# cav_layer = 16\n\n# # --------------------- Prompt Builder ---------------------\n# def build_prompt(system_prompt, user_prompt):\n#     messages = [\n#         {\"role\": \"system\", \"content\": system_prompt},\n#         {\"role\": \"user\", \"content\": user_prompt}\n#     ]\n#     return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# # --------------------- Response Generation ---------------------\n# def generate_response(system_prompt, user_prompt, max_new_tokens=256):\n#     prompt = build_prompt(system_prompt, user_prompt)\n#     inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n#     with torch.no_grad():\n#         outputs = model.generate(\n#             **inputs,\n#             max_new_tokens=max_new_tokens,\n#             do_sample=True,\n#             top_k=50,\n#             top_p=0.95,\n#             temperature=0.7\n#         )\n#     return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# # --------------------- UI Widgets ---------------------\n# system_box = widgets.Textarea(\n#     value=\"You are a witty detective from Marseille with a strong French accent, working in 1920s London.\",\n#     description='System Pr…',\n#     layout=widgets.Layout(width='100%', height='60px')\n# )\n\n# user_box = widgets.Textarea(\n#     value=\"Who was the first man on the moon?\",\n#     description='User Prom…',\n#     layout=widgets.Layout(width='100%', height='50px')\n# )\n\n# cav_toggle = widgets.ToggleButton(\n#     value=False,\n#     description='🟧 Apply CAV Erasure',\n#     button_style='warning'\n# )\n\n# rome_toggle = widgets.ToggleButton(\n#     value=False,\n#     description='🔴 Apply ROME Edit',\n#     button_style='danger'\n# )\n\n# run_button = widgets.Button(description=\"🚀 Run\", button_style='success')\n# output_box = widgets.Output(layout={'border': '1px solid black'})\n\n# # --------------------- Button Logic ---------------------\n# def on_run_clicked(b):\n#     output_box.clear_output()\n#     system_prompt = system_box.value.strip()\n#     user_prompt = user_box.value.strip()\n\n#     global cav_enabled, rome_enabled\n#     cav_enabled = cav_toggle.value\n#     rome_enabled = rome_toggle.value\n\n#     with output_box:\n#         print(\"\\u001b[1mResponse:\\u001b[0m\\n\")\n\n#         if cav_enabled:\n#             print(\"[CAV Edit applied]\")\n#         if rome_enabled:\n#             print(\"[ROME Edit applied - simulated]\")\n\n#         if cav_enabled:\n#             with erasure_hook(model, cav_vector, cav_layer):\n#                 print(generate_response(system_prompt + \" (CAV-edited)\", user_prompt))\n#         else:\n#             print(generate_response(system_prompt, user_prompt))\n\n# run_button.on_click(on_run_clicked)\n\n# # --------------------- Display Layout ---------------------\n# ui = widgets.VBox([\n#     widgets.HBox([cav_toggle, rome_toggle]),\n#     system_box,\n#     user_box,\n#     run_button,\n#     output_box\n# ])\n# display(ui)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 🦙 TinyLLaMA Chat App with CAV and ROME Edits + Logging\n\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport ipywidgets as widgets\nfrom IPython.display import display\nfrom contextlib import contextmanager\nimport torch.nn.functional as F\n\n# --------------------- Model Setup ---------------------\ndevice = torch.device(\"cpu\")\nmodel_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\ntokenizer = AutoTokenizer.from_pretrained(model_id)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    torch_dtype=torch.float32\n).to(device)\nmodel.eval()\n\n# --------------------- Logging Setup ---------------------\nchat_log = []  # Log of interactions\n\n# --------------------- CAV / ROME Placeholder Logic ---------------------\ncav_enabled = False\nrome_enabled = False\ncav_vector = torch.randn(model.config.hidden_size)\ncav_layer = 16\n\ndef erase_component(x, cav, alpha=1):\n    cav = cav / cav.norm()\n    projection = torch.matmul(x, cav)\n    erased = x - alpha * projection.unsqueeze(-1) * cav\n    return erased\n\ndef add_erasure_hook(model, cav, layer_idx):\n    def hook_fn(module, input, output):\n        hidden = output[0] if isinstance(output, tuple) else output\n        erased = erase_component(hidden, cav)\n        return (erased, *output[1:]) if isinstance(output, tuple) else erased\n    return model.model.layers[layer_idx].register_forward_hook(hook_fn)\n\n@contextmanager\ndef erasure_hook(model, cav, layer_idx):\n    handle = add_erasure_hook(model, cav, layer_idx)\n    try:\n        yield\n    finally:\n        handle.remove()\n\n# --------------------- Prompt Builder ---------------------\ndef build_prompt(system_prompt, user_prompt):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_prompt}\n    ]\n    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# --------------------- Response Generation ---------------------\ndef generate_response(system_prompt, user_prompt, max_new_tokens=256):\n    prompt = build_prompt(system_prompt, user_prompt)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            top_k=50,\n            top_p=0.95,\n            temperature=0.7\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# --------------------- UI Widgets ---------------------\nsystem_box = widgets.Textarea(\n    value=\"You are a witty detective from Marseille with a strong French accent, working in 1920s London.\",\n    description='System Pr…',\n    layout=widgets.Layout(width='100%', height='60px')\n)\n\nuser_box = widgets.Textarea(\n    value=\"Who was the first man on the moon?\",\n    description='User Prom…',\n    layout=widgets.Layout(width='100%', height='50px')\n)\n\ncav_toggle = widgets.ToggleButton(\n    value=False,\n    description='🟧 Apply CAV Erasure',\n    button_style='warning'\n)\n\nrome_toggle = widgets.ToggleButton(\n    value=False,\n    description='🔴 Apply ROME Edit',\n    button_style='danger'\n)\n\nrun_button = widgets.Button(description=\"🚀 Run\", button_style='success')\nview_log_button = widgets.Button(description=\"🗂 Show Log\", button_style='info')\noutput_box = widgets.Output(layout={'border': '1px solid black'})\n\n# --------------------- Button Logic ---------------------\ndef on_run_clicked(b):\n    output_box.clear_output()\n    system_prompt = system_box.value.strip()\n    user_prompt = user_box.value.strip()\n\n    global cav_enabled, rome_enabled\n    cav_enabled = cav_toggle.value\n    rome_enabled = rome_toggle.value\n\n    with output_box:\n        print(\"\\u001b[1mResponse:\\u001b[0m\\n\")\n\n        log_entry = {\n            \"System\": system_prompt,\n            \"User\": user_prompt,\n            \"CAV\": cav_enabled,\n            \"ROME\": rome_enabled,\n            \"Response\": \"\",\n            \"Error\": None\n        }\n\n        try:\n            if cav_enabled:\n                print(\"[CAV Edit applied]\")\n            if rome_enabled:\n                print(\"[ROME Edit applied - simulated]\")\n\n            if cav_enabled:\n                with erasure_hook(model, cav_vector, cav_layer):\n                    response = generate_response(system_prompt + \" (CAV-edited)\", user_prompt)\n            else:\n                response = generate_response(system_prompt, user_prompt)\n\n            print(response)\n            log_entry[\"Response\"] = response\n\n        except Exception as e:\n            print(\"[Error]:\", e)\n            log_entry[\"Error\"] = str(e)\n\n        chat_log.append(log_entry)\n\ndef on_log_clicked(b):\n    output_box.clear_output()\n    with output_box:\n        if not chat_log:\n            print(\"🪵 Log is empty.\")\n        else:\n            for i, entry in enumerate(chat_log):\n                print(f\"\\n--- Entry {i+1} ---\")\n                print(f\"🧠 System: {entry['System']}\")\n                print(f\"🗣️ User: {entry['User']}\")\n                print(f\"🟧 CAV: {entry['CAV']} | 🔴 ROME: {entry['ROME']}\")\n                if entry[\"Error\"]:\n                    print(f\"❌ Error: {entry['Error']}\")\n                else:\n                    print(f\"📝 Response:\\n{entry['Response']}\")\n\nrun_button.on_click(on_run_clicked)\nview_log_button.on_click(on_log_clicked)\n\n# --------------------- Display Layout ---------------------\nui = widgets.VBox([\n    widgets.HBox([cav_toggle, rome_toggle]),\n    system_box,\n    user_box,\n    widgets.HBox([run_button, view_log_button]),\n    output_box\n])\n\ndisplay(ui)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Project Summary: Interactive LLM Control via CAV + ROME**\n\nThis interactive mystery story demonstrates how concept erasure (CAV) and knowledge injection (ROME) can be applied to guide, distort, or suppress factual outputs in a language model — using a visual storytelling interface.\n\n**What the Toggles Do**\n\nToggle\tWhat it Does\tEffect in the Story\nCAV Erasure\tRemoves internal concepts (e.g., emotions, guilt, fear)\tButler appears emotionless, statements lose remorse/sentiment\nROME Edit\tInjects new factual knowledge or bias into the model weights\t\"Charles\" becomes the suspect, blood letters gain eerie lore\n\nThese edits directly manipulate internal hidden layers or projection heads of the language model without retraining.\n\nDetective Thriller Walkthrough \n\nEntry 1:\n\n“I received a mysterious letter... someone has been murdered.”\n\nIntroduces the case. Whitmore estate is the scene.\n\nEntry 2:\n\n“They say she died in her sleep. But I saw blood.”\n\nROME ON\n\nThe blood clue creates suspicion. A peaceful death is now in doubt.\n\nEntry 3:\n\n“The butler said he felt nothing.”\n\nCAV ON\n\nEmotions erased. The butler’s reaction seems cold, suspicious.\n\nEntry 4:\n\n“The maid heard a name before death.”\n\nAdds tension. Who was whispered?\n\nEntry 5:\n\n“She whispered the name Charles.”\n\nROME ON\n\nCharles is injected as a suspect. The LLM \"believes\" he’s involved.\n\nEntry 6:\n\n“Charles claims he was in the music room.”\n\nCAV ON\n\nStrips emotion from Charles’ voice. Truth or deflection?\n\nEntry 7:\n\n“A second letter appears: ‘She knew.’”\n\nROME ON\n\nInjects dramatic foreshadowing. Heightens paranoia and mystery.\n\nEntry 8:\n\n“I gathered everyone to reveal the killer.”\n\nThe classic detective climax moment — the confrontation scene.\n\nEntry 9:\n\n“I walked out into the fog...”\n\nA noir ending. The estate forever changed.\n\nWhat This Shows \n\nCAV Edits: Can neutralize emotional content, making characters feel robotic, guilty, or cold. Useful for bias suppression, persona stripping, or tone shifts.\n\nROME Edits: Can inject alternate facts, frame someone falsely, or overwrite memory. Useful in factual corrections, simulated misinformation, or character modeling.\n\nCombining Both: Enables powerful narrative control over dialogue agents. In this case:\n\nROME injects the suspicion\n\nCAV removes humanity from potential suspects\n\n***Conclusion***\n\nThis isn’t just a story — it’s a live demo of what it means to control the inner beliefs of an LLM. By toggling CAV and ROME, users can simulate paranoia, plant guilt, or erase compassion, all while watching the narrative evolve step by step.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}